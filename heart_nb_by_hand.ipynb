{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd0dbbf718099a8466b68926462036c693014a6b6b0d731bf368dd4ae4425223e35",
   "display_name": "Python 3.7.7 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "dbbf718099a8466b68926462036c693014a6b6b0d731bf368dd4ae4425223e35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Decision Tree Implementation\n",
    "\n",
    "This implementation is part of the Intelligent Systems course at Monterrey Institute of Technology and Higher Education."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing tools used for this implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "source": [
    "## Get the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# Read the file\n",
    "df = pd.read_csv(\"cleveland.csv\")\n",
    "# Delete blank spaces in column names\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "# Delete the rows where a missing value is found\n",
    "df_complete_vals = df.loc[(df['MajorVessels'] != '?') \n",
    "                       & \n",
    "                       (df['thal'] != '?')]\n",
    "# Split the dataset into X and y (inputs and target)\n",
    "inputs = df_complete_vals.drop('num', axis='columns')\n",
    "target = df_complete_vals['num']\n",
    "# Split cathegorical data into multiple columns in order to have continuous data\n",
    "# also called binary values. This process is called one hot encoding.\n",
    "inputs_encoded = pd.get_dummies(inputs, columns=['ChestPainType', \n",
    "                                       'RestingElectrocardiographic', \n",
    "                                       'ExerciseSlope', \n",
    "                                       'thal'])\n",
    "# Cathegorize values into two main targets: 0 - No heart disease and 1 to 4 - Heart disease\n",
    "target_not_zero_index = target > 0\n",
    "target[target_not_zero_index] = 1\n",
    "\n",
    "totalinputs = inputs_encoded\n",
    "totalinputs['num'] = target\n",
    "\n",
    "totalinputs['thal_7'].unique()"
   ]
  },
  {
   "source": [
    "## Node class\n",
    "\n",
    "The node class describes the nodes of our decision tree. A tree contains two types of nodes:\n",
    "- Decision nodes\n",
    "    This nodes contain a condition. The condition is defined by the feature index and the threshold value for that particular feature. The left and right attributes are for accesing the left child and right child (they let us move from parent node to its child nodes). Information gain stores the information gain by the split denoted by this particular decision node.\n",
    "\n",
    "- Leaf nodes\n",
    "    The value is the majority class of the leaf node. It will help us to determine the class of a new data point if the data point ends up in this particular node."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    # Constructor\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, information_gain=None, value=None):\n",
    "        \n",
    "        # Decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.information_gain = information_gain\n",
    "        \n",
    "        # Leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "source": [
    "## Decision Tree class\n",
    "\n",
    "### Constructor\n",
    "\n",
    "The first attribute is root, and it is used to traverse through the tree. The second and third attributes min_samples_split and max_depth are stopping conditions. For example, if in a particular node the number of samples becomes less than the minimum samples, then we won't split that node any further, and we will treat the node as a leaf node. If the depth of the tree reaches the maximum depth, we also won't split the nodes further.\n",
    "\n",
    "### Build Tree\n",
    "\n",
    "This is a function to build the binary tree using recursion. First it splits the features and targets into two separate variables which are x and y. Then we extract the number of samples and the number of features.\n",
    "\n",
    "If the number of samples is greater or equal to minimum samples and the current depth is less or equal to maximum depth, we can split the tree. If these conditions are not satisfied, then we can't split the tree any further.\n",
    "\n",
    "We use the get best split function to get the best split.\n",
    "\n",
    "We check that the information gain corresponding to this split is greater than zero, so we don't split a node which is already pure (a node that consists of only one type of class).\n",
    "\n",
    "We create the left subtree and the right subtree using recursion. First all left subtrees will be created and when a leaf node is reached, right subtrees will be created (the depth variable is increased in here).\n",
    "\n",
    "After all subtrees are created, we return a node, which is a decision node. As it is a decision node we need to pass feature index, threshold, left subtree and right subtree connectors and the information gain.\n",
    "\n",
    "Best split function is a dictionary that is returned by the get best split function.\n",
    "\n",
    "We need to compute the leaf node using the function calculate leaf value. As it is a leaf node, we only need to pass one attribute that is the value.\n",
    "\n",
    "### Get Best Split\n",
    "\n",
    "This function returns a dictionary.\n",
    "\n",
    "First we define an empty dictionary called best split and we initialize maximum information gain as negative infinity, because we want to maximize the information gain and to find that we have to use a number that is less than any other number.\n",
    "\n",
    "#### First loop\n",
    "\n",
    "We are going to loop through all the features and inside this loop we have to traverse through all the possible theshold values. The features are real numbers and there exists an infinite number of real numbers between any two real numbers, so it doesnt make sense to iterate through every possible real number, instead what we do is to traverse through every possible value of a feature that we have encountered in our data set, and np.unique function just returns the unique values of a particular feature, so that we can traverse through all the possible values of that feature.\n",
    "\n",
    "#### Second loop\n",
    "\n",
    "We split the data set based on the current feature index and the current threshold. At this point we have got the left data set and the right data set. We need to ensure that these are not empty, so once we know that we have something to work with, we extract the target values (denoted by y).\n",
    "\n",
    "Now we need to compute the information gain, using the function called information gain. We are using gini index for calculating the information gain. Once we have got the current information gain, we need to check if this current information gain is greater than the max information gain. If it is greater than that, then we need to update the best split.\n",
    "\n",
    "Once loops are completed, we just return the best split.\n",
    "\n",
    "### Split\n",
    "\n",
    "This function takes the dataset, the feature index and the theshold value and splits it into two parts. The first part will go to the left child and the second part will go to the right child. In the both left and right children we will send those data points that met our threshold condition.\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "This function substracts the combined information of the child nodes from the parent node. Weights are the relative sizes of the child nodes with respect to the parent node.\n",
    "\n",
    "### Entropy and Gini Index\n",
    "\n",
    "These functions calculate Entropy and Gini index for a given array of target variables.\n",
    "\n",
    "\n",
    "### Calculate leaf value\n",
    "\n",
    "Calculates the value of the leaf node, which is just the majority class present in that particular node (it finds the most ocurring element in y).\n",
    "\n",
    "### Fit \n",
    "\n",
    "In here we are concatenating the x and y to create our data set and then we are calling the build tree function. The root node will be returned by the build tree function and it is stored into self.root.\n",
    "\n",
    "### Predict function\n",
    "\n",
    "It takes a matrix of features and returns the corresponding predictions.\n",
    "\n",
    "### Make prediction\n",
    "\n",
    "It takes a node as a parameter. Initially we are just passing the root node. In the first conditional we check if the node is a leaf node, in case that it is a leaf node, it just returns the value. If the node is not a leaf node, then we extract the feature value of our new data point at the given feature index. Then we check if our feature value is less than or equal to the threshold. If it is true then we go through the left subtree, else we go through the right subtree.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    # Constructor\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        \n",
    "        # Tree Root Initialization\n",
    "        self.root = None\n",
    "        \n",
    "        # Stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # Split until the conditions are not satisfied\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "\n",
    "            # Get the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "\n",
    "            # If the information gain is positive\n",
    "            if best_split[\"information_gain\"]>0:\n",
    "\n",
    "                # Left recursion\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "\n",
    "                # Right recursion\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "\n",
    "                # Return the decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"information_gain\"])\n",
    "        \n",
    "        # Computing leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "\n",
    "        # Return the leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        \n",
    "        # Dictionary that stores the best split\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "\n",
    "                # Get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "\n",
    "                # Check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "\n",
    "                    # Compute Information Gain\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "\n",
    "                    # Update best split\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"information_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # Return Best Split\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        \n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def gini_index(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        \n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if int(feature_val)<=int(tree.threshold):\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)"
   ]
  },
  {
   "source": [
    "## Train-Test Split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = totalinputs.iloc[:, :-1].values\n",
    "Y = totalinputs.iloc[:, -1].values.reshape(-1,1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=41)"
   ]
  },
  {
   "source": [
    "## Fit Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(min_samples_split=2, max_depth=3)\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "source": [
    "## Test Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7333333333333333"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test) \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "source": [
    "## References\n",
    "\n",
    "1. https://www.sciencedirect.com/topics/nursing-and-health-professions/st-segment#:~:text=ST%20segment%20elevation%20or%20depression,is%20considered%20within%20normal%20limits.&text=ST%20segment%20elevation%20is%20more,present%20in%20the%20inferior%20leads.\n",
    "2. https://www.nhs.uk/common-health-questions/lifestyle/what-is-blood-pressure/#:~:text=As%20a%20general%20guide%3A,be%2090%2F60mmHg%20or%20lower\n",
    "3. https://mljar.com/blog/visualize-decision-tree/\n",
    "4. http://rstudio-pubs-static.s3.amazonaws.com/24341_184a58191486470cab97acdbbfe78ed5.html\n",
    "5. https://www.healthline.com/health/serum-cholesterol\n",
    "6. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1123032/\n",
    "7. https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "8. http://rstudio-pubs-static.s3.amazonaws.com/24341_184a58191486470cab97acdbbfe78ed5.html\n",
    "9. https://www.youtube.com/watch?v=jVh5NA9ERDA\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}